<!DOCTYPE html>




<html class="theme-next gemini" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="https://picture-krona.oss-cn-shenzhen.aliyuncs.com/Picture/K.svg?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="https://picture-krona.oss-cn-shenzhen.aliyuncs.com/Picture/K.svg?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="https://picture-krona.oss-cn-shenzhen.aliyuncs.com/Picture/K.svg?v=5.1.4">


  <link rel="mask-icon" href="https://picture-krona.oss-cn-shenzhen.aliyuncs.com/Picture/K.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="深度学习," />










<meta name="description" content="深度学习计算相关知识笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习计算">
<meta property="og:url" content="http://example.com/2024/03/06/6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/index.html">
<meta property="og:site_name" content="Kayy">
<meta property="og:description" content="深度学习计算相关知识笔记">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-03-06T09:30:23.000Z">
<meta property="article:modified_time" content="2024-03-08T05:05:53.692Z">
<meta property="article:author" content="KayLL">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/2024/03/06/6深度学习计算/"/>





  <title>深度学习计算 | Kayy</title>
  








<meta name="generator" content="Hexo 7.1.1"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Kayy</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Welcome to my kingdom</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/home/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            Schedule
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/03/06/6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kayy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">深度学习计算</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-03-06T17:30:23+08:00">
                2024-03-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          
              <div class="post-description">
                  深度学习计算相关知识笔记
              </div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="深度学习计算">5. 深度学习计算</h1>
<h2 id="层和块">5.1 层和块</h2>
<h2 id="参数管理">5.2 参数管理</h2>
<p><span class="math display">\[
w\sim \left\{
\begin{array}{l@{\quad}r}
U(5,10)&amp;\text{可能性$\frac 1 4$}\\
0&amp;\text{可能性$\frac 1 2$}\\
U(-10,-5)&amp;\text{可能性$\frac 1 4$}
\end{array}
\right.
\tag{5.2.1}
\]</span></p>
<p>选择架构并设置了超参数之后，我便进入训练阶段。目标是使得算是函数最小化的模型参数值，经过训练后，使用这些参数来做出未来的预测。</p>
<p>另外，有时也需要提取参数以便在其他环境中复用</p>
<ul>
<li>访问参数，用于调试、诊断和可视化</li>
<li>参数初始化</li>
<li>在不同模型共享参数</li>
</ul>
<p>首先看具有单隐藏层的多层感知器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">net=nn.Sequential(nn.Linear(<span class="number">4</span>,<span class="number">8</span>),nn.ReLU(),nn.Linear(<span class="number">8</span>,<span class="number">1</span>))</span><br><span class="line">X=torch.rand(size(<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line">net(x)</span><br></pre></td></tr></table></figure>
<h3 id="参数访问">5.2.1 参数访问</h3>
<p>我们从已有的的模型访问参数例如</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>]state_dict())</span><br></pre></td></tr></table></figure>
<p>也可以</p>
<p>如果直接<code>print(net)</code>那么</p>
<blockquote>
<p>Sequential( (0): Linear(in_features=4, out_features=8, bias=True)
(1): ReLU() (2): Linear(in_features=8, out_features=1, bias=True) )</p>
</blockquote>
<p>当然也可以</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(net)):</span><br><span class="line">    <span class="built_in">print</span>(net[i].state_dict())</span><br></pre></td></tr></table></figure>
<blockquote>
<p>OrderedDict([('weight', tensor([[-0.0986, 0.2455, -0.0860, 0.0680],
[-0.1355, 0.1110, -0.2777, -0.2812], [ 0.0556, -0.1747, 0.4240, 0.0055],
[ 0.4844, -0.2566, -0.2516, -0.0411], [-0.4566, -0.2377, -0.0310,
-0.1946], [-0.1937, 0.4383, -0.2517, 0.3974], [ 0.3601, 0.0696, -0.1477,
-0.1017], [ 0.0562, 0.1064, -0.1053, -0.0964]])), ('bias', tensor([
0.1770, 0.3808, -0.0480, -0.2350, -0.1038, -0.3598, 0.3858, 0.2915]))])
OrderedDict() OrderedDict([('weight', tensor([[ 0.2666, -0.3127,
-0.2842, -0.0652, -0.0895, 0.2961, -0.1265, -0.0633]])), ('bias',
tensor([-0.2793]))])</p>
</blockquote>
<p>可以见的如果每一层网络都有权重bias,前一层<code>m*n</code>那么后一层为<code>n*k</code></p>
<p>显然这个模型的输出为1个,输入为4个</p>
<h4 id="目标参数">5.2.1.1 目标参数</h4>
<p>每个参数都表示为参数类的一个实例.要对参数执行任何操作,首先我们需要访问底层参数的数值。</p>
<p>例如</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net[<span class="number">2</span>].bias))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.data)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>&lt;class 'torch.nn.parameter.Parameter'&gt; Parameter containing:
tensor([-0.2793], requires_grad=True) tensor([-0.2793])</p>
</blockquote>
<p>参数是符合的对象，<strong>包含值、梯度、和额外信息</strong></p>
<h4 id="一次访问所有参数">5.2.1.2 一次访问所有参数</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(*[(name,param.shape) <span class="keyword">for</span> name,param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters()])</span><br><span class="line"><span class="built_in">print</span>(*[(name,param.shape) <span class="keyword">for</span> name,param <span class="keyword">in</span> net.named_parameters()])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>('weight', torch.Size([8, 4])) ('bias', torch.Size([8])) ('0.weight',
torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight',
torch.Size([1, 8])) ('2.bias', torch.Size([1]))</p>
</blockquote>
<h4 id="从嵌套块中收集参数">5.2.1.3 从嵌套块中收集参数</h4>
<p>如果将多个块相互嵌套，参数命名约定是如何工作的。</p>
<p>首先定义一个“块工厂”，然后将这些块组合到更大的块中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">block1</span>():</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(nn.Linear(<span class="number">4</span>,<span class="number">8</span>),nn.ReLU(),nn.Linear(<span class="number">8</span>,<span class="number">4</span>),nn.ReLU())</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">block2</span>():</span><br><span class="line">    net-nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        net.add_module(<span class="string">f&#x27;block<span class="subst">&#123;i&#125;</span>&#x27;</span>,block1())</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line">rgnet=nn.Sequential(block2(),nn.Linear(<span class="number">4</span>,<span class="number">1</span>))</span><br><span class="line">rgnet(X)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([[-0.1032], [-0.1033]], grad_fn=<AddmmBackward>)</p>
</blockquote>
<blockquote>
<p>block1构建包含2个线性层，2个非线性relu，4*4，</p>
<p>block2通过循环构建4个（block1)+一个线性层，最终为4*1，网络层数17</p>
<p>最后将X通过该网络得到输出</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(rgnet)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Sequential( (0): Sequential( (block0): Sequential( (0):
Linear(in_features=4, out_features=8, bias=True) (1): ReLU() (2):
Linear(in_features=8, out_features=4, bias=True) (3): ReLU() ) (block1):
Sequential( (0): Linear(in_features=4, out_features=8, bias=True) (1):
ReLU() (2): Linear(in_features=8, out_features=4, bias=True) (3): ReLU()
) (block2): Sequential( (0): Linear(in_features=4, out_features=8,
bias=True) (1): ReLU() (2): Linear(in_features=8, out_features=4,
bias=True) (3): ReLU() ) (block3): Sequential( (0):
Linear(in_features=4, out_features=8, bias=True) (1): ReLU() (2):
Linear(in_features=8, out_features=4, bias=True) (3): ReLU() ) ) (1):
Linear(in_features=4, out_features=1, bias=True) )</p>
</blockquote>
<h3 id="参数初始化">5.2.2 参数初始化</h3>
<p>默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵，这个范围根据输入输出维度计算得来</p>
<h4 id="内置初始化">5.2.2.1 内置初始化</h4>
<p>将所有权重参数初始化为标准差为0.01的高斯随机变量，且偏置参数设置为0</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_normal</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">type</span>(m))==nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight,mean=<span class="number">0</span>,std=<span class="number">0.01</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">net.apply(init_normal)</span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>],net[<span class="number">0</span>].bias.data[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>若设置常数，则为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_constant</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">net.apply(init_constant)</span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>], net[<span class="number">0</span>].bias.data[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<h4 id="自定义初始化">5.2.2.2 自定义初始化</h4>
<p>有时候深度学习框架没有提供我们的初始化方法，<strong>使用我们设定的特定分布来为任意权重参数<span
class="math inline">\(w\)</span>定义初始化方法</strong>： $$</p>
<p>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_init</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Init&quot;</span>, *[(name, param.shape)</span><br><span class="line">                        <span class="keyword">for</span> name, param <span class="keyword">in</span> m.named_parameters()][<span class="number">0</span>])</span><br><span class="line">        nn.init.uniform_(m.weight, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        m.weight.data *= m.weight.data.<span class="built_in">abs</span>() &gt;= <span class="number">5</span></span><br><span class="line">		<span class="comment">#上一行即为自定义参数</span></span><br><span class="line">net.apply(my_init)</span><br><span class="line">net[<span class="number">0</span>].weight[:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<h3 id="参数绑定">5.2.3 参数绑定</h3>
<p>希望在多个层之间共享参数</p>
<p><strong>可以定义一个稠密层，然后使用它的参数来设置另一个层参数</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们需要给共享层一个名称，以便可以引用它的参数</span></span><br><span class="line">shared = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">net(X)</span><br><span class="line"><span class="comment"># 检查参数是否相同</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line">net[<span class="number">2</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="comment"># 确保它们实际上是同一个对象，而不只是有相同的值</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<h2 id="延后初始化">5.3 延后初始化</h2>
<p>深度学习框架无法判断网络的输入维度是什么$$延后初始化(defers
initialization)：</p>
<p>数据第一次通过模型传递时，框架才会动态地推断出每层的大小</p>
<h2 id="自定义层">5.4 自定义层</h2>
<h3 id="不带参数的层">5.4.1 不带参数的层</h3>
<p>首先我们构建一个没有任何参数的自定义层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> f</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CenteredLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,X</span>):</span><br><span class="line">        <span class="keyword">return</span> X-X.mean()</span><br><span class="line">layer = CenteredLayer()</span><br><span class="line">layer(torch.FloatTensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([-2., -1., 0., 1., 2.])</p>
</blockquote>
<p>现在，我们可以将层作为组件合并到更复杂的模型中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net=nn.Sequential(nn.Linear(<span class="number">8</span>,<span class="number">128</span>),CenteredLayer())</span><br><span class="line">Y = net(torch.rand(<span class="number">4</span>, <span class="number">8</span>))</span><br><span class="line">Y.mean()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor(-9.3132e-09, grad_fn=<MeanBackward0>)</p>
</blockquote>
<blockquote>
<p>解释:</p>
<p>创建自定义层，对经过的样本的每个特征数据减去mean，确保E为0</p>
<p>然后将一个8*128的全连接层，将8个特征映射成128个</p>
<p>创建net,然后将随机生成的具有8个特征的4个样本，经过网络，求均值</p>
</blockquote>
<h3 id="带参数的层">5.4.2 带参数的层</h3>
<p><strong>全连接</strong>:<strong>权重</strong>、<strong>偏置</strong></p>
<p>in_units,units分别表述输入数，输出数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_units,units</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight=nn.Parameter(torch.randn(in_units,units))</span><br><span class="line">        self.bias=nn.Parameter(torch.randn(units,))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,X</span>):</span><br><span class="line">        <span class="comment">#Y=XW+b</span></span><br><span class="line">        linear=torch.matmul(X,self.weight.data)+self.bias.data</span><br><span class="line">        <span class="keyword">return</span> F.relu(linear)</span><br><span class="line">linear=MyLinear(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">linear.weightpy</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Parameter containing: tensor([[-1.9495, -0.6294, 0.2030], [ 0.6147,
0.1423, 0.9114], [ 0.0044, -0.2396, 0.0970], [ 1.2450, 1.0536, 0.4277],
[ 0.7913, -0.5266, 2.0368]], requires_grad=True)</p>
</blockquote>
<p>使用自定层进行前向传播计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linear(torch.rand(<span class="number">10</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([[0.1387, 1.0461, 3.2200], [0.0000, 0.8358, 1.6702], [0.0000,
1.0186, 1.4001], [0.1124, 1.4258, 2.9308], [0.2609, 0.7889, 2.3094],
[0.7560, 1.1164, 2.2255], [0.2433, 0.5997, 3.4911], [1.4168, 1.4965,
3.3127], [0.4558, 1.6162, 2.4079], [0.0000, 0.8643, 1.9909]])</p>
</blockquote>
<h2 id="读写文件">5.5 读写文件</h2>
<h3 id="加载和保存张量">5.5.1 加载和保存张量</h3>
<p>对于单个张量，可以直接调用load和save函数分别读写。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line">X=torch.arange(<span class="number">4</span>)</span><br><span class="line">torch.save(X,<span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line">x2=torch.load(<span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line">x2</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([0, 1, 2, 3])</p>
</blockquote>
<h3 id="加载和保存模型参数">5.5.2 加载和保存模型参数</h3>
<p>构建多层感知器，然后创建实例，保存参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.output(F.relu(self.hidden(x)))</span><br><span class="line"></span><br><span class="line">net = MLP()</span><br><span class="line">X = torch.randn(size=(<span class="number">2</span>, <span class="number">20</span>))</span><br><span class="line">Y = net(X)</span><br><span class="line">torch.save(net.state_dict(), <span class="string">&#x27;mlp.params&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>创建一个实例，读取参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">clone = MLP()</span><br><span class="line">clone.load_state_dict(torch.load(<span class="string">&#x27;mlp.params&#x27;</span>))</span><br><span class="line">clone.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>MLP( (hidden): Linear(in_features=20, out_features=256, bias=True)
(output): Linear(in_features=256, out_features=10, bias=True) )</p>
</blockquote>
<h2 id="gpu">5.6 GPU</h2>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2024/03/05/%E5%85%AC%E5%BC%8F%E8%A7%A3%E5%86%B3/" rel="next" title="关于hexo博客的公式显示问题解决方案">
                <i class="fa fa-chevron-left"></i> 关于hexo博客的公式显示问题解决方案
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2024/03/08/%E8%B4%9D%E5%8F%B6%E6%96%AF/" rel="prev" title="基于贝叶斯分类器的相关基础知识分析">
                基于贝叶斯分类器的相关基础知识分析 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20archive">
              
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/krona12" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="krona126139@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97"><span class="nav-text">5. 深度学习计算</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B1%82%E5%92%8C%E5%9D%97"><span class="nav-text">5.1 层和块</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E7%AE%A1%E7%90%86"><span class="nav-text">5.2 参数管理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E8%AE%BF%E9%97%AE"><span class="nav-text">5.2.1 参数访问</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E5%8F%82%E6%95%B0"><span class="nav-text">5.2.1.1 目标参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E6%AC%A1%E8%AE%BF%E9%97%AE%E6%89%80%E6%9C%89%E5%8F%82%E6%95%B0"><span class="nav-text">5.2.1.2 一次访问所有参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8E%E5%B5%8C%E5%A5%97%E5%9D%97%E4%B8%AD%E6%94%B6%E9%9B%86%E5%8F%82%E6%95%B0"><span class="nav-text">5.2.1.3 从嵌套块中收集参数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-text">5.2.2 参数初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%85%E7%BD%AE%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-text">5.2.2.1 内置初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-text">5.2.2.2 自定义初始化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E7%BB%91%E5%AE%9A"><span class="nav-text">5.2.3 参数绑定</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BB%B6%E5%90%8E%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-text">5.3 延后初始化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82"><span class="nav-text">5.4 自定义层</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E5%B8%A6%E5%8F%82%E6%95%B0%E7%9A%84%E5%B1%82"><span class="nav-text">5.4.1 不带参数的层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%A6%E5%8F%82%E6%95%B0%E7%9A%84%E5%B1%82"><span class="nav-text">5.4.2 带参数的层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6"><span class="nav-text">5.5 读写文件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BF%9D%E5%AD%98%E5%BC%A0%E9%87%8F"><span class="nav-text">5.5.1 加载和保存张量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="nav-text">5.5.2 加载和保存模型参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gpu"><span class="nav-text">5.6 GPU</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">KayLL</span>

  
</div>








<div class="powered-by">
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <i class="fa fa-user-md"></i>
    <span id="busuanzi_container_site_uv">
        本站访客数:<span id="busuanzi_value_site_uv"></span>
    </span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_pv">
        本站访问量<span id="busuanzi_value_site_pv"></span>
    </span>
</div>



        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  


  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  
  <script>
    $('.highlight').each(function (i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap')
      $(e).after($wrap)
      $wrap.append($('<button>').addClass('copy-btn').append('copy').on('click', function (e) {
        var code = $(this).parent().find('.code').find('.line').map(function (i, e) {
          return $(e).text()
        }).toArray().join('\n')  // Corrected the line break symbol here
        var ta = document.createElement('textarea')
        document.body.appendChild(ta)
        ta.style.position = 'absolute'
        ta.style.top = '0px'
        ta.style.left = '0px'
        ta.value = code
        ta.select()
        ta.focus()
        var result = document.execCommand('copy')
        document.body.removeChild(ta)
        
          if(result)$(this).text('done')
          else $(this).text('failure')
        
        $(this).blur()
      })).on('mouseleave', function (e) {
        var $b = $(this).find('.copy-btn')
        setTimeout(function () {
          $b.text('copy')
        }, 300)
      }).append(e)
    })
  </script>


</body>
</html>






